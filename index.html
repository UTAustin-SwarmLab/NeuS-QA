<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="NeuS-QA: Grounding Long-Form Video Understanding in Temporal Logic and Neuro-Symbolic Reasoning">
  <meta name="keywords" content="video, understanding, reasoning, neuro-symbolic, ai, temporal, logic, formal, methods">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>NeuS-QA: Grounding Long-Form Video Understanding in Temporal Logic and Neuro-Symbolic Reasoning</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png" type="image/png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <style>
    @media only screen and (max-width: 768px) {
      body {
        margin: 20px;
      }
    }
  </style>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://utaustin-swarmlab.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://utaustin-swarmlab.github.io/nsvs/">
            NSVS
          </a>
          <a class="navbar-item" href="https://utaustin-swarmlab.github.io/NeuS-V">
            NeuS-V
          </a>
          <a class="navbar-item" href="https://utaustin-swarmlab.github.io/NeuS-QA">
            NeuS-QA
          </a>
        </div>
      </div>
    </div>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">NeuS-QA: Grounding Long-Form Video Understanding in Temporal Logic and Neuro-Symbolic Reasoning</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Sahil Shah<sup>1</sup>,</span>
            <span class="author-block">
              S P Sharan<sup>&dagger; 1</sup>,</span>
            <span class="author-block">
              Harsh Goel<sup>&dagger; 1</sup>,</span>
            <span class="author-block">
              Minkyu Choi<sup>1</sup>,</span>
            <span class="author-block">
              Mustafa Munir<sup>1</sup>,</span>
            <span class="author-block">
              Manvik Pasula<sup>2</sup>,</span>
            <span class="author-block">
              Radu Marculescu<sup>1</sup>,</span>
            <span class="author-block">
              Sandeep Chinchali<sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>The University of Texas at Austin, USA</span>
            <span class="author-block"><sup>2</sup>Independent Researcher, USA</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">&dagger;Contributed equally to this work</span>
          </div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2509.18041"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2509.18041"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/UTAustin-SwarmLab/NeuS-QA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Source Code</span>
                  </a>
              </span>

            </div>
        </div>
        <br></br>
        <div class="columns is-centered">
          <div class="column is-two-thirds">
            <img src="static/images/teaser.png" class="zoomable-image">
          </div>
        </div>
    </div>
  </div>
  <span style="display: block; height: 36px;"></span>
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method Overview</h2>
        <div class="content has-text-justified">
          <p>
            While vision-language models (VLMs) excel at tasks involving single images or short videos, they still struggle with Long Video Question Answering (LVQA) due to its demand for complex multi-step temporal reasoning. Vanilla approaches, which simply sample frames uniformly and feed them to a VLM along with the question, incur significant token overhead. This forces aggressive downsampling of long videos, causing models to miss fine-grained visual structure, subtle event transitions, and key temporal cues. Recent works attempt to overcome these limitations through heuristic approaches; however, they lack explicit mechanisms for encoding temporal relationships and fail to provide any formal guarantees that the sampled context actually encodes the compositional or causal logic required by the question.
          </p>
          <p>
            To address these foundational gaps, we introduce NeuS-QA, a training-free, plug-and-play neuro-symbolic pipeline for LVQA. NeuS-QA first translates a natural language question into a logic specification that models the temporal relationship between frame-level events. Next, we construct a video automaton to model the video's frame-by-frame event progression, and finally employ model checking to compare the automaton against the specification to identify all video segments that satisfy the question's logical requirements. Only these logic-verified segments are submitted to the VLM, thus improving interpretability, reducing hallucinations, and enabling compositional reasoning without modifying or fine-tuning the model. Experiments on the LongVideoBench and CinePile LVQA benchmarks show that NeuS-QA significantly improves performance by over 10%, particularly on questions involving event ordering, causality, and multi-step reasoning.
          </p>
        </div>
        <div class="content has-text-centered">
          <div class="columns is-centered">
            <div class="column is-three-quarters">
              <img src="static/images/running_example.png" class="zoomable-image">
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<br>
<div class="container is-max-desktop">
  <div class="row">
    <h2 class="title is-3 has-text-centered">Key Capabilities</h2>
  </div>
  <br>
  <h3 class="title is-4 has-text-centered">Quantitative Results</h3>
  <p>
  NeuS-QA outperforms existing state-of-the-art LVQA methods, including both foundation models and structured reasoning frameworks, by a significant margin on the LongVideoBench benchmark. NeuS-QA particularly excelling in questions that require complex temporal reasoning.
  </p>
  <br>
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <img src="static/images/main_table.png" class="zoomable-image">
      </div>
  </div>

  <div class="columns is-centered">
    <div class="column">
      <div class="content">
        <h3 class="title is-4 has-text-centered">Generalization to Narrative Video</h3>
        <p>
        NeuS-QA demonstrates strong generalization to narrative video domains, such as movies and TV shows, as shown by it outperforming all other models on the CinePile benchmark.
        </p>
      </div>
    </div>
    <div class="column">
      <div class="columns is-centered">
        <div class="column content">
          <h3 class="title is-4 has-text-centered">Impact of VLM Backbone</h3>
          <p>
          NeuS-QA is model-agnostic and works with any VLM backbone. Using InternVL2-8B for automaton construction results in the highest accuracy when evaluated with GPT-4o.
          </p>
        </div>
      </div>
    </div>
  </div>
  <div class="columns is-centered">
    <div class="column">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <img src="static/images/narrative.png" class="zoomable-image">
          </div>
        </div>
      </div>
    </div>
    <div class="column">
      <div class="columns is-centered">
        <div class="column content">
          <img src="static/images/backbone.png" class="zoomable-image">
        </div>
      </div>
    </div>
  </div>
</div>
<br>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{shah2025neus,
  title     = {NeuS-QA: Grounding Long-Form Video Understanding in Temporal Logic and Neuro-Symbolic Reasoning},
  author    = {Shah, Sahil and Sharan, SP and Goel, Harsh and Choi, Minkyu and Munir, Mustafa and Pasula, Manvik and Marculescu, Radu and Chinchali, Sandeep},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  year      = {2026}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
          <p>
            Website source based on <a href="https://github.com/nerfies/nerfies.github.io">this source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


<script>
  window.addEventListener("load", () => {
    const items = document.querySelectorAll(".results-carousel .item");
    let maxHeight = 0;

    items.forEach(item => {
      item.style.height = "auto"; // reset to measure properly
    });

    items.forEach(item => {
      const height = item.getBoundingClientRect().height;
      if (height > maxHeight) maxHeight = height;
    });

    items.forEach(item => {
      item.style.height = `${maxHeight}px`;
    });
  });
</script>

<script>
  document.addEventListener("DOMContentLoaded", () => {
    const images = document.querySelectorAll(".zoomable-image");
    const zoomOverlay = document.createElement("div");
    zoomOverlay.classList.add("zoom-overlay");
    const zoomImage = document.createElement("img");
    zoomOverlay.appendChild(zoomImage);
    document.body.appendChild(zoomOverlay);
    images.forEach((img) => {
      const highResSrc = img.getAttribute("data-highres");
      if (highResSrc) {
        const preloadImg = new Image();
        preloadImg.src = highResSrc;
      }

      img.addEventListener("click", () => {
        zoomImage.src = highResSrc || img.src;
        zoomOverlay.classList.add("active");
      });
    });
    zoomOverlay.addEventListener("click", () => {
      zoomOverlay.classList.remove("active");
      zoomImage.src = "";
    });
    document.addEventListener("keydown", (e) => {
      if (e.key === "Escape") zoomOverlay.classList.remove("active");
    });
  });
</script>

</body>
</html>


